---
title: "AE2"
author: "Marco, Celeste, Guillermo"
date: "2026-01-12"
output: html_document
---

```{r carga_librerias}
library(httr)
library(XML)
library(knitr)
library(DT)
library(dplyr)
library(ggplot2)
library(stringr)
```


# Pregunta 1
```{r p1_1}
url <- "https://www.mediawiki.org/wiki/MediaWiki"

#1.1
  #Descargar la página
  respuesta <- GET(url)

  #Extraer el HTML como texto
  html_raw <- content(respuesta, as = "text", encoding = "UTF-8")

  #Parsear el HTML a un documento XML el cual se llama html_doc
  html_doc <- htmlTreeParse(html_raw, useInternalNodes = TRUE)

  #Ver la clase del objeto
  class(html_doc)
```


## 1.2
```{r p1_2}
titulo <- xpathSApply(html_doc, "//title", xmlValue)
titulo

```


## 1.3
```{r p1_3}
  #Extraer Texto del enlace
  textos <- xpathSApply(html_doc, "//a", xmlValue)
  
  #Extraer los URL
  urls <- xpathSApply(html_doc, "//a/@href")
  
  #handle nules
  nulos_texto <- sapply(textos, is.null)
  nulos_url   <- sapply(urls, is.null)
  
  textos[nulos_texto] <- NA
  urls[nulos_url]     <- NA
  
  textos <- unlist(textos)
  urls   <- unlist(urls)

  enlaces <- data.frame(
  texto = textos,
  url   = urls,
  stringsAsFactors = FALSE
)

datatable(head(enlaces))
```

## 1.4

```{r}
# Paso 1-2: Contar y convertir a dataframe
conteo_urls <- as.data.frame(table(enlaces$url))

# Paso 3: Renombrar
names(conteo_urls) <- c("url", "frecuencia")

# Paso 4: Unir con textos
tabla_enlaces <- merge(enlaces, conteo_urls, by = "url")

# Paso 5: Eliminar duplicados
tabla_enlaces_unicos <- tabla_enlaces[!duplicated(tabla_enlaces$url), ]

# Paso 6: Ordenar
tabla_enlaces_unicos <- tabla_enlaces_unicos[order(-tabla_enlaces_unicos$frecuencia), ]

# Paso 7: Seleccionar columnas
tabla_final <- tabla_enlaces_unicos[, c("texto", "url", "frecuencia")]

# Paso 8: Renumerar filas desde 1
rownames(tabla_final) <- 1:nrow(tabla_final)

# ============================================
# TABLAS FORMATEADAS (para R Markdown)
# ============================================


# Top 10 enlaces
kable(head(tabla_final, 10), caption = "Top 10 Enlaces más Frecuentes")
# Resumen de verificación
resumen_verificacion <- data.frame(
  Metrica = c("Total URLs únicas", "Suma de frecuencias", "URLs con frecuencia > 1", "URLs únicas (frecuencia = 1)"),
  Valor = c(nrow(tabla_final), sum(tabla_final$frecuencia), sum(tabla_final$frecuencia > 1), sum(tabla_final$frecuencia == 1))
)
kable(resumen_verificacion, caption = "Verificación de Resultados")

# Distribución de frecuencias
distribucion <- as.data.frame(table(tabla_final$frecuencia))
names(distribucion) <- c("Frecuencia", "Cantidad_URLs")
kable(distribucion, caption = "Distribución de Frecuencias")
```
```{r}
#1.5

normalizar_url <- function(u) {
  if (is.na(u) || u == "") return(NA)
  if (grepl("^#", u)) return(url)
  if (grepl("^https?://", u)) return(u)
  if (grepl("^//", u)) return(paste0("https:", u))
  if (grepl("^/", u)) return(paste0("https://www.mediawiki.org", u))
  return(NA)
}

comprobar_estado <- function(u) {
  if (is.na(u)) return(NA)
  tryCatch({
    res <- HEAD(u, timeout(5))
    Sys.sleep(1)
    return(status_code(res))
  }, error = function(e) {
    Sys.sleep(1)
    return(NA)
  })
}

tabla_final$url_normalizada <- sapply(tabla_final$url, normalizar_url)
tabla_final$status_code <- sapply(tabla_final$url_normalizada, comprobar_estado)

kable((tabla_final),caption = "Enlaces con estado HTTP")
```

## 2.1
```{r p2_1}
# Inicialmente clasificaremos las url si son absolutas o relativas y obtendremos la frecuencia con la que aparecen.
df_abs_rel <- enlaces %>%
  mutate(
    tipo_url = if_else(
      str_detect(url, "^https?://"),
      "URL absoluta",
      "URL relativa"
    )
  ) %>%
  count(tipo_url, name = "frecuencia")

# Luego generamos la gráfica de barras donde separa la frecuencia de URL absoluta y relativa.
ggplot(df_abs_rel, aes(x = tipo_url, y = frecuencia, fill = tipo_url)) +
  geom_col(width = 0.6) +
  labs(
    title = "Frecuencia de aparición de URLs",
    x = "Tipo de URL",
    y = "Cantidad de enlaces"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## 2.2
```{r p2_2}

# Definimos el dominio raiz o principal

dominio_principal <- "https://www.mediawiki.org"

# Continuamos clasificando los enlaces que tenemos entre los que apuntan hacia la misma URL y las que apuntan hacia un destino externo.

df_ext_int <- enlaces %>%
  mutate(
    tipo_enlace = case_when(
      # URLs relativas → internas
      !str_detect(url, "^https?://") ~ "Interno",

      # URLs absolutas que apuntan al dominio principal → internas
      str_detect(url, fixed(dominio_principal)) ~ "Interno",

      # URLs absolutas a otros dominios → externas
      TRUE ~ "Externo"
    )
  )

# Ahora hacemos la suma de la cantidad de enlaces por tipo

df_resumen_tipo <- df_ext_int %>%
  count(tipo_enlace, name = "total_enlaces")

# Finalmente generamos el gráfico de barras solicitado.

ggplot(df_resumen_tipo, aes(x = tipo_enlace, y = total_enlaces, fill = tipo_enlace)) +
  geom_col(width = 0.6) +
  labs(
    title = "Distribución de enlaces internos vs externos",
    x = "Tipo de enlace",
    y = "Cantidad total de enlaces"
  ) +
  theme_minimal() +
  theme(
    legend.position = "none"
  )
```

## 2.3
```{r p2_3}

# Inicialmente realizamos el conteo de cada uno de los codigos de estado.

df_status <- tabla_final %>%
  count(status_code, name = "frecuencia") %>%
  mutate(
    porcentaje = frecuencia / sum(frecuencia) * 100,
    etiqueta = paste0(status_code, " (", round(porcentaje, 1), "%)")
  )

# Generamos el gráfico de pastel que indica los porcentajes de peticiones.

pie_chart <- ggplot(df_status, aes(x = "", y = frecuencia, fill = factor(status_code))) +
  geom_col(width = 1, color = "red") +
  coord_polar(theta = "y") +
  geom_text(aes(label = etiqueta), position = position_stack(vjust = 0.5), size = 4) +
  labs(
    title = "Distribución de Status de Enlaces",
    fill = "Status"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    panel.grid = element_blank()
  )
pie_chart
```