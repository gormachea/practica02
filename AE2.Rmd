---
title: "AE2"
author: "Marco, Celeste, Guillermo"
date: "2026-01-12"
output: html_document
---

```{r carga_librerias}
library(httr)
library(XML)
library(knitr)
library(DT)
```


# Pregunta 1
```{r p1_1}
url <- "https://www.mediawiki.org/wiki/MediaWiki"

#1.1
  #Descargar la página
  respuesta <- GET(url)

  #Extraer el HTML como texto
  html_raw <- content(respuesta, as = "text", encoding = "UTF-8")

  #Parsear el HTML a un documento XML el cual se llama html_doc
  html_doc <- htmlTreeParse(html_raw, useInternalNodes = TRUE)

  #Ver la clase del objeto
  class(html_doc)
```


## 1.2
```{r p1_2}
titulo <- xpathSApply(html_doc, "//title", xmlValue)
titulo

```


## 1.3
```{r p1_3}
  #Extraer Texto del enlace
  textos <- xpathSApply(html_doc, "//a", xmlValue)
  
  #Extraer los URL
  urls <- xpathSApply(html_doc, "//a/@href")
  
  #handle nules
  nulos_texto <- sapply(textos, is.null)
  nulos_url   <- sapply(urls, is.null)
  
  textos[nulos_texto] <- NA
  urls[nulos_url]     <- NA
  
  textos <- unlist(textos)
  urls   <- unlist(urls)

  enlaces <- data.frame(
  texto = textos,
  url   = urls,
  stringsAsFactors = FALSE
)

datatable(head(enlaces))
```

## 1.4

```{r}
# Paso 1-2: Contar y convertir a dataframe
conteo_urls <- as.data.frame(table(enlaces$url))

# Paso 3: Renombrar
names(conteo_urls) <- c("url", "frecuencia")

# Paso 4: Unir con textos
tabla_enlaces <- merge(enlaces, conteo_urls, by = "url")

# Paso 5: Eliminar duplicados
tabla_enlaces_unicos <- tabla_enlaces[!duplicated(tabla_enlaces$url), ]

# Paso 6: Ordenar
tabla_enlaces_unicos <- tabla_enlaces_unicos[order(-tabla_enlaces_unicos$frecuencia), ]

# Paso 7: Seleccionar columnas
tabla_final <- tabla_enlaces_unicos[, c("texto", "url", "frecuencia")]

# Paso 8: Renumerar filas desde 1
rownames(tabla_final) <- 1:nrow(tabla_final)

# ============================================
# TABLAS FORMATEADAS (para R Markdown)
# ============================================


# Top 10 enlaces
kable(head(tabla_final, 10), caption = "Top 10 Enlaces más Frecuentes")
# Resumen de verificación
resumen_verificacion <- data.frame(
  Metrica = c("Total URLs únicas", "Suma de frecuencias", "URLs con frecuencia > 1", "URLs únicas (frecuencia = 1)"),
  Valor = c(nrow(tabla_final), sum(tabla_final$frecuencia), sum(tabla_final$frecuencia > 1), sum(tabla_final$frecuencia == 1))
)
kable(resumen_verificacion, caption = "Verificación de Resultados")

# Distribución de frecuencias
distribucion <- as.data.frame(table(tabla_final$frecuencia))
names(distribucion) <- c("Frecuencia", "Cantidad_URLs")
kable(distribucion, caption = "Distribución de Frecuencias")
```
```{r}
#1.5

normalizar_url <- function(u) {
  if (is.na(u) || u == "") return(NA)
  if (grepl("^#", u)) return(url)
  if (grepl("^https?://", u)) return(u)
  if (grepl("^//", u)) return(paste0("https:", u))
  if (grepl("^/", u)) return(paste0("https://www.mediawiki.org", u))
  return(NA)
}

comprobar_estado <- function(u) {
  if (is.na(u)) return(NA)
  tryCatch({
    res <- HEAD(u, timeout(5))
    Sys.sleep(1)
    return(status_code(res))
  }, error = function(e) {
    Sys.sleep(1)
    return(NA)
  })
}

tabla_final$url_normalizada <- sapply(tabla_final$url, normalizar_url)
tabla_final$status_code <- sapply(tabla_final$url_normalizada, comprobar_estado)

kable((tabla_final),caption = "Enlaces con estado HTTP")
```
